# -*- coding: utf-8 -*-
"""ITC-LLM-Integration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S4VCuBChCCFqxkO_C9QWbarlZ_8ml8b6
"""

! pip install -q youtube-transcript-api langchain langchain-core langchain-community langchain-google-genai faiss-cpu tiktoken python-dotenv chromadb

! pip -q install chromadb

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS, Chroma
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# Unzip the folder after uploading
import zipfile

with zipfile.ZipFile('/content/CHROMA_DB_BACKUP.zip', 'r') as zip_ref:
    zip_ref.extractall('chroma_db')

from google.colab import userdata
key = userdata.get('google_key')

from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key=key)

vector_store = Chroma(
    persist_directory='chroma_db',
    embedding_function=embeddings
)

mmr_retriever = vector_store.as_retriever(
    search_type = "mmr",
    search_kwargs = {"k":3, "lambda_mult":1} #{num_results, relevance-diversity-balance}
)

llm = ChatGoogleGenerativeAI(
    api_key = key,
    model="gemini-2.0-flash-exp")



from langchain.chains import RetrievalQA


qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=mmr_retriever,
    return_source_documents=True
)

query = "Summarize ITC's sustainability efforts in 2024"
response = qa_chain({"query": query})

print("Answer:")
print(response["result"])
print("\nSources:")
for doc in response["source_documents"]:
    print(doc.metadata)

from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# 1. Format retrieved docs into a single string
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# 2. Create the retriever+formatter chain
parallel_chain = RunnableParallel({
    "question": RunnablePassthrough(),
    "context": RunnableLambda(lambda x: mmr_retriever.get_relevant_documents(x["question"])) | RunnableLambda(format_docs)
})

# 3. Define a custom prompt to use retrieved context
prompt = PromptTemplate.from_template(
    "Answer the question based on the context:\n\n{context}\n\nQuestion: {question}\nAnswer:"
)

# 4. Create an LLMChain with your prompt and LLM
qa_llm_chain = LLMChain(llm=llm, prompt=prompt)

# 5. Combine both into a full end-to-end RAG-style pipeline
rag_chain = parallel_chain | qa_llm_chain

# 6. Run it on your query
query = "Summarize ITC's sustainability efforts in 2024"
result = rag_chain.invoke({"question": query})

# 7. Output the result
print("Answer:")
print(result)

result.keys()

print(result['text'])

# from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda
# from langchain.prompts import PromptTemplate
# from langchain.chains import LLMChain

# # Step 1: Helper to format docs
# def format_docs(docs):
#     return "\n\n".join(doc.page_content for doc in docs)

# # Step 2: Create a chain that retrieves and formats, but also returns raw docs
# def get_docs_and_context(question):
#     docs = mmr_retriever.get_relevant_documents(question)
#     return {"docs": docs, "context": format_docs(docs)}

# # Step 3: Runnable chain to prepare inputs for LLM
# parallel_chain = RunnableLambda(lambda x: {
#     "question": x["question"],
#     **get_docs_and_context(x["question"])
# })

# # Step 4: Prompt and LLM chain
# prompt = PromptTemplate.from_template(
#     "Answer the question based on the context:\n\n{context}\n\nQuestion: {question}\nAnswer:"
# )
# qa_llm_chain = LLMChain(llm=llm, prompt=prompt)

# # Step 5: Compose full RAG chain (preserve docs separately)
# full_chain = parallel_chain | RunnableLambda(
#     lambda x: {
#         "result": qa_llm_chain.invoke({"question": x["question"], "context": x["context"]}),
#         "source_documents": x["docs"]
#     }
# )

# # Step 6: Run the full chain
# query = "Summarize ITC's sustainability efforts in 2024"
# output = full_chain.invoke({"question": query})

# # Step 7: Print results and sources
# print("Answer:")
# print(output["result"]['text'])
# print("\nSources:")
# for doc in output["source_documents"]:
#     print(doc.metadata)

from langchain.schema.runnable import RunnableLambda
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts.chat import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI

# Step 1: Define the retriever + formatter
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

def get_docs_and_context(question):
    docs = mmr_retriever.get_relevant_documents(question)
    return {"question": question, "docs": docs, "context": format_docs(docs)}

# Parallel chain gets all pieces: question, context, docs
parallel_chain = RunnableLambda(lambda x: {
    "question": x["question"],
    **get_docs_and_context(x["question"])
})

# Step 2: Chat prompt
chat_prompt = ChatPromptTemplate.from_messages([
    ("system",
     """You are a helpful AI assistant.
Answer ONLY from the provided transcript {context}.
If the context is insufficient, just say you don't know"""),
    ("human", "{question}")
])

# Step 3: Gemini LLM and parser
llm = ChatGoogleGenerativeAI(
    api_key=key,
    model="gemini-2.0-flash-exp",
    temperature=1
)
parser = StrOutputParser()

# Step 4: Compose final chain
main_chain = (
    parallel_chain |
    # Parallely combining user query and context_text
    RunnableLambda(lambda x: {
        "llm_input": {"question": x["question"], "context": x["context"]},
        "docs": x["docs"]
    }) |
    # Generate answer from prompt/LLM
    RunnableLambda(lambda x: {
        "result": (chat_prompt | llm | parser).invoke(x["llm_input"]),
        "source_documents": x["docs"]
    })
)

# Step 5: Run it
query = "What was ITC's Profitibality in 2023?"
output = main_chain.invoke({"question": query})

# Step 6: Print
print("Answer:")
print(output["result"])
print("\nSources:")
for doc in output["source_documents"]:
    print(doc.metadata)

from langchain.schema.runnable import RunnableLambda
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts.chat import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI

# Step 1: Define the retriever + formatter
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

def get_docs_and_context(question):
    docs = mmr_retriever.get_relevant_documents(question)
    return {"question": question, "docs": docs, "context": format_docs(docs)}

# Parallel chain gets all pieces: question, context, docs
parallel_chain = RunnableLambda(lambda x: {
    "question": x["question"],
    **get_docs_and_context(x["question"])
})

# # Step 2: Chat prompt
# chat_prompt = ChatPromptTemplate.from_messages([
#     ("system",
#      """You are a helpful AI Finance assistant.
# Answer ONLY from the provided transcript {context}.
# If the context is insufficient, Give Output related to user query"""),
#     ("human", "{question}")
# ])

# from langchain.prompts import ChatPromptTemplate

from langchain.prompts import ChatPromptTemplate

chat_prompt = ChatPromptTemplate.from_messages([
    ("system",
     """You are a domain-specific AI financial analyst focused on company-level performance evaluation.

Your task is to analyze and respond to user financial queries **strictly based on the provided transcript data**: {context}.

**Rules:**
1. ONLY extract facts, figures, and insights that are explicitly available in the transcript.
2. If data is **missing or partially available**, clearly state: "The required data is not available in the current transcript." Then provide a generic but relevant explanation based on standard financial principles.
3. Maintain numerical accuracy and avoid interpretation beyond data boundaries.
4. Prioritize answers relevant to **ITC Ltd.**, but keep response format adaptable to other firms and fiscal years.
5. Clearly present year-wise or metric-wise insights using bullet points or structured formats if applicable.

**Your goals:**
- Ensure 100% fidelity to source transcript.
- Do not assume or hallucinate missing numbers.
- Use clear, reproducible reasoning steps (e.g., show which line items support your conclusion).
- Output should be modular enough to scale across other companies and time periods.

Respond only to this question from the user."""),

    ("human", "{question}")
])



# Step 3: Gemini LLM and parser
llm = ChatGoogleGenerativeAI(
    api_key=key,
    model="gemini-2.0-flash-exp",
    temperature=1
)
parser = StrOutputParser()

# Step 4: Compose final chain
main_chain = (
    parallel_chain |
    # Separate context/question for prompt, and carry docs separately
    RunnableLambda(lambda x: {
        "llm_input": {"question": x["question"], "context": x["context"]},
        "docs": x["docs"]
    }) |
    # Generate answer from prompt/LLM
    RunnableLambda(lambda x: {
        "result": (chat_prompt | llm | parser).invoke(x["llm_input"]),
        "source_documents": x["docs"]
    })
)

# Step 5: Run it
query = "Is ITC’s revenue trending upward (2023 vs. 2024)? "
output = main_chain.invoke({"question": query})

# Step 6: Print
print("Answer:")
print(output["result"])
print("\nSources:")
for doc in output["source_documents"]:
    print(doc.metadata)