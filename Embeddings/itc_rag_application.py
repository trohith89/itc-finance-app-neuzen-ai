# -*- coding: utf-8 -*-
"""ITC-RAG-Application.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n1Rv5sh1BXuBm42EVUq9c4SLf95nXPJd

# ITC Financial Analysis Rag Application

## Main Components:
 1. Document Loaders
 2. Text Splitterss
 3. Vector Stores
 4. Retrievers
"""

! pip install -q youtube-transcript-api langchain langchain-core langchain-community langchain-google-genai faiss-cpu tiktoken python-dotenv

!pip install -q chromadb

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS, Chroma
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

"""## Step 1-a : Indexing : Document Loading"""

import pickle
from langchain.schema import Document

with open("/content/documents_Final (1).pkl", "rb") as f:
    loaded_docs = pickle.load(f)

len(loaded_docs)

"""## Step 1-b : Indexing : Text Splitting"""

# RecursiveCharacterTextSplitter based Chunking
splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)
chunks = splitter.split_documents(loaded_docs)

len(chunks)

s = 0
for doc in loaded_docs:
    s+= len(doc.page_content)
    # print(len(doc.page_content))
s

"""## Step 1-c & 1-d : Indexing(Embedding Generation and Storing in Vector Store)

"""

from google.colab import userdata
key = userdata.get('google_key')

from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key=key)

vector_store = Chroma.from_documents(chunks, embedding=embeddings, persist_directory="CHROMA_DB")

import shutil

shutil.make_archive('CHROMA_DB_BACKUP', 'zip', 'CHROMA_DB')

"""## Step 2 : Retriever -> Using Maximum Marginal Retriever(Helps with more diversity retrieval)"""

mmr_retriever = vector_store.as_retriever(
    search_type = "mmr",
    search_kwargs = {"k":3, "lambda_mult":1} #{num_results, relevance-diversity-balance}
)

llm = ChatGoogleGenerativeAI(
    api_key = key,
    model="gemini-2.0-flash-exp")

from langchain.chains import RetrievalQA


qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=mmr_retriever,
    return_source_documents=True
)

from langchain.chains import RetrievalQA


qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=mmr_retriever,
    return_source_documents=True
)

query = "What was the total return to shareholders in 2024?"
response = qa_chain({"query": query})

print("Answer:")
print(response["result"])
print("\nSources:")
for doc in response["source_documents"]:
    print(doc.metadata)